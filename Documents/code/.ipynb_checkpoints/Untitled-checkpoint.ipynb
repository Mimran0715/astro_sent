{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f7d4a5e6c47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;31m# lda link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.gensim'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re \n",
    "import sqlite3 \n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion() # take away popups - putting plot images in pdf\n",
    "#import pdfplumber\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.cluster import AgglomerativeClustering\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn import feature_extraction # br\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer # lsi\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import random\n",
    "import seaborn as sns \n",
    "from pprint import pprint # lda link\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('/Users/Mal/Documents/research.db')\n",
    "cur = conn.cursor()\n",
    "df_1 = pd.read_sql_query(\"SELECT  bibcode, title, year, author, abstract, citation_count FROM astro_papers_2 WHERE year >= 1980 AND year <1990 LIMIT 200;\", conn)\n",
    "df_2 = pd.read_sql_query(\"SELECT  bibcode, title, year, author, abstract, citation_count FROM astro_papers_2 WHERE year >= 1990 AND year <2000 LIMIT 200;\", conn)\n",
    "df_3 = pd.read_sql_query(\"SELECT  bibcode, title, year, author, abstract, citation_count FROM astro_papers_2 WHERE year >= 2000 AND year <2010 LIMIT 200;\", conn)\n",
    "df_4 = pd.read_sql_query(\"SELECT  bibcode, title, year, author, abstract, citation_count FROM astro_papers_2 WHERE year >= 2010 AND year <2022 LIMIT 200;\", conn)\n",
    "df = pd.concat([df_1, df_2, df_3, df_4], axis=0)\n",
    "#print(\"DF INFO --------->\")\n",
    "#print(df.info(verbose=False, memory_usage=\"deep\"))\n",
    "#print()\n",
    "#kmeans_clustering(df, results_dir, folder_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "    return re.findall(\"[a-zA-Z0-9']{2,}\", text)\n",
    "\n",
    "def tokenize(text):\n",
    "    return [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "\n",
    "def tokenize_bigram(text):\n",
    "    tokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "    return list(ngrams(tokens, 2))\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems  # taken from: http://brandonrose.org/clustering_mobile\n",
    "    \n",
    "def process_text(df):\n",
    "    #stop_words = stopwords.words('english')\n",
    "    abs_text = df['abstract']\n",
    "    abs_text = abs_text.str.lower() \n",
    "    #abs_text = abs_text.apply(parse_text)\n",
    "    #print(abs_text) \n",
    "\n",
    "    abs_text = abs_text.apply(parse_text)\n",
    "    abs_text = abs_text.apply(' '.join)\n",
    "\n",
    "    #abs_text = abs_text.apply(stemmer.stem())\n",
    "\n",
    "    abs_text_stem = abs_text.apply(tokenize_and_stem)\n",
    "    abs_text_token = abs_text.apply(tokenize)\n",
    "    abs_text_bigram = abs_text.apply(tokenize_bigram)\n",
    "    #abs_text_trigram = abs_text.apply(tokenize_trigram)\n",
    "\n",
    "    #print(df.columns)\n",
    "    #print(abs_text_bigram)\n",
    "    #df['abs_text'] = abs_text_stem #? \n",
    "    df['abs_text'] = abs_text_token\n",
    "    #df['abs_text'] = abs_text_bigram\n",
    "    \n",
    "    \n",
    "    # tokenized = []\n",
    "    # for abs_text in df['abs_text']:\n",
    "    #     abstract_str = \" \".join(abs_text)\n",
    "    #     tokenized.append(abstract_str)\n",
    "\n",
    "    # df['abs_tokenized'] = tokenized\n",
    "\n",
    "    # for abs_text in abs_text_token:\n",
    "    #     for term in abs_text:\n",
    "    #         vocab_tokens[term] +=1 \n",
    "\n",
    "    # for abs_text in abs_text_stem:\n",
    "    #     for term in abs_text:\n",
    "    #         vocab_stems[term] +=1 \n",
    "\n",
    "    # for abs_text in abs_text_bigram:\n",
    "    #     for term in abs_text:\n",
    "    #         vocab_bigrams[term] += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##GET LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = process_text(df)\n",
    "train_2, test_2 = train_test_split(df_2, train_size=0.75)\n",
    "\n",
    "texts = list(train_2['abs_text'])\n",
    "id2word = Dictionary(texts)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                    id2word=id2word,\n",
    "                    num_topics=10, \n",
    "                    random_state=100,\n",
    "                    update_every=1,\n",
    "                    chunksize=100,\n",
    "                    passes=10,\n",
    "                    alpha='auto',\n",
    "                    per_word_topics=True)\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
